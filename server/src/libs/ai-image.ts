import { env } from '@/config/env.js';
import { uploadFile } from './storage.js';
import { logger } from './logger.js';
import { InternalError, BadRequestError } from '@/shared/errors/errors.js';
import type { StorageFile } from './storage.js';

export interface AiImageResult {
  urls: string[];
}

/**
 * Process an image with AI. Delegates to the provider configured in AI_PROVIDER env var.
 */
export async function processImageWithAI(
  imageBuffer: Buffer,
  prompt: string,
  count: number = env.OPENAI_IMAGE_COUNT,
): Promise<AiImageResult> {
  const provider = env.AI_PROVIDER;
  logger.info({ provider, promptLength: prompt.length, count }, 'Starting AI image processing');

  if (provider === 'gemini') {
    return processWithGemini(imageBuffer, prompt, count);
  }
  return processWithOpenAI(imageBuffer, prompt, count);
}

// ── Gemini Provider ──

async function processWithGemini(
  imageBuffer: Buffer,
  prompt: string,
  count: number,
): Promise<AiImageResult> {
  const { getGeminiClient } = await import('./gemini.js');
  const gemini = getGeminiClient();

  try {
    const base64Image = imageBuffer.toString('base64');
    const urls: string[] = [];

    // Gemini generates one image per call, so loop for count
    for (let i = 0; i < count; i++) {
      const response = await gemini.models.generateContent({
        model: env.GEMINI_IMAGE_MODEL,
        contents: [
          {
            role: 'user',
            parts: [
              { text: prompt },
              {
                inlineData: {
                  mimeType: 'image/png',
                  data: base64Image,
                },
              },
            ],
          },
        ],
        config: {
          responseModalities: ['Text', 'Image'],
        },
      });

      const candidates = response.candidates;
      if (!candidates || candidates.length === 0) {
        logger.warn({ index: i }, 'Gemini returned no candidates');
        continue;
      }

      const parts = candidates[0].content?.parts;
      if (!parts) {
        logger.warn({ index: i }, 'Gemini returned no parts');
        continue;
      }

      for (const part of parts) {
        if (part.inlineData?.data) {
          const buffer = Buffer.from(part.inlineData.data, 'base64');
          const mimeType = part.inlineData.mimeType ?? 'image/png';
          const ext = mimeType.includes('jpeg') ? '.jpg' : '.png';
          const storageFile: StorageFile = {
            buffer,
            filename: `result-${i}${ext}`,
            mimetype: mimeType,
          };
          const url = await uploadFile(storageFile, 'results');
          urls.push(url);
        }
      }
    }

    if (urls.length === 0) {
      throw new InternalError('No images were generated by Gemini');
    }

    return { urls };
  } catch (err: unknown) {
    if (err instanceof InternalError || err instanceof BadRequestError) {
      throw err;
    }
    const errMsg = err instanceof Error ? err.message : String(err);
    if (errMsg.includes('SAFETY') || errMsg.includes('blocked')) {
      throw new BadRequestError('Image was rejected by content safety policy', 'CONTENT_POLICY_VIOLATION');
    }
    if (errMsg.includes('429') || errMsg.includes('RESOURCE_EXHAUSTED')) {
      logger.warn({ err }, 'Gemini rate limit hit');
      throw new InternalError('Image processing temporarily unavailable. Please try again shortly.');
    }
    logger.error({ err }, 'Gemini API error');
    throw new InternalError('Image processing failed');
  }
}

// ── OpenAI Provider ──

async function processWithOpenAI(
  imageBuffer: Buffer,
  prompt: string,
  count: number,
): Promise<AiImageResult> {
  const { openai } = await import('./openai.js');
  const { toFile } = await import('openai');
  const OpenAI = (await import('openai')).default;

  try {
    const file = await toFile(imageBuffer, 'input.png', { type: 'image/png' });

    const response = await openai.images.edit({
      model: env.OPENAI_IMAGE_MODEL,
      image: file,
      prompt,
      n: count,
      size: env.OPENAI_IMAGE_SIZE as '1024x1024' | '512x512' | '256x256',
    });

    const urls: string[] = [];
    const responseData = response.data;

    if (!responseData || responseData.length === 0) {
      throw new InternalError('No images were generated by OpenAI');
    }

    for (let i = 0; i < responseData.length; i++) {
      const item = responseData[i];
      let buffer: Buffer;

      if (item.b64_json) {
        buffer = Buffer.from(item.b64_json, 'base64');
      } else if (item.url) {
        const fetchRes = await fetch(item.url);
        if (!fetchRes.ok) {
          logger.warn({ index: i, status: fetchRes.status }, 'Failed to fetch OpenAI result image');
          continue;
        }
        buffer = Buffer.from(await fetchRes.arrayBuffer());
      } else {
        logger.warn({ index: i }, 'OpenAI returned no image data for variant');
        continue;
      }

      const storageFile: StorageFile = {
        buffer,
        filename: `result-${i}.png`,
        mimetype: 'image/png',
      };
      const url = await uploadFile(storageFile, 'results');
      urls.push(url);
    }

    if (urls.length === 0) {
      throw new InternalError('No images were generated by OpenAI');
    }

    return { urls };
  } catch (err: unknown) {
    if (err instanceof OpenAI.APIError) {
      if (err.status === 429) {
        logger.warn({ err }, 'OpenAI rate limit hit');
        throw new InternalError('Image processing temporarily unavailable. Please try again shortly.');
      }
      if (err.status === 400 && err.message?.includes('content_policy')) {
        throw new BadRequestError('Image was rejected by content safety policy', 'CONTENT_POLICY_VIOLATION');
      }
      logger.error({ err, status: err.status }, 'OpenAI API error');
      throw new InternalError('Image processing failed');
    }
    if (err instanceof InternalError || err instanceof BadRequestError) {
      throw err;
    }
    logger.error({ err }, 'Unexpected error during OpenAI image processing');
    throw new InternalError('Image processing failed');
  }
}
